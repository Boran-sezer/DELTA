import streamlit as st
from groq import Groq
import firebase_admin
from firebase_admin import credentials, firestore
import base64, json, hashlib
from datetime import datetime

# --- INITIALISATION FIREBASE ---
if not firebase_admin._apps:
    try:
        encoded = st.secrets["firebase_key"]["encoded_key"].strip()
        decoded_json = base64.b64decode(encoded).decode("utf-8")
        cred_dict = json.loads(decoded_json)
        cred = credentials.Certificate(cred_dict)
        firebase_admin.initialize_app(cred)
    except Exception as e:
        st.error(f"Erreur Firebase : {e}")
        st.stop()

db = firestore.client()
USER_ID = "monsieur_sezer"

# --- INITIALISATION GROQ ---
client = Groq(api_key="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")

# --- UTILITAIRES M√âMOIRE ---
def hash_text(text: str) -> str:
    return hashlib.sha256(text.encode("utf-8")).hexdigest()

def is_memory_worthy(text: str) -> bool:
    """D√©cide si une information m√©rite d'√™tre m√©moris√©e, fa√ßon Jarvis."""
    # blacklist simple pour √©viter les trivialit√©s
    blacklist = ["salut", "ok", "mdr", "lol", "?", "oui", "non"]
    if len(text.strip()) < 15 or any(word in text.lower() for word in blacklist):
        return False

    # V√©rification via Groq (LLM) pour d√©cider si info est utile
    try:
        analysis = client.chat.completions.create(
            model="llama-3.3-70b-versatile",
            messages=[
                {"role": "system", "content": "Tu es Jarvis, assistant de Tony Stark. "
                                              "D√©cide si cette info m√©rite d'√™tre m√©moris√©e. "
                                              "R√©ponds strictement en JSON : {'is_worthy': bool, 'priority': 'high|medium|low', 'branch':'nom_de_branche'}"},
                {"role": "user", "content": text}
            ],
            response_format={"type": "json_object"}
        )
        res = json.loads(analysis.choices[0].message.content)
        return res
    except:
        return {"is_worthy": False, "priority": "low", "branch": "G√©n√©ral"}

def get_recent_memories(limit=10):
    """R√©cup√®re les souvenirs r√©cents pour contextualiser Jarvis"""
    memories = []
    try:
        mem_ref = db.collection("users").document(USER_ID).collection("memory")
        docs = mem_ref.order_by("created_at", direction=firestore.Query.DESCENDING).limit(limit).stream()
        memories = [d.to_dict() for d in docs]
    except:
        pass
    return memories

def summarize_context(memories, max_chars=500):
    """R√©sume les souvenirs r√©cents pour fournir un contexte LLM"""
    lines = []
    for m in memories:
        lines.append(f"[{m.get('priority','medium')}] {m.get('content')}")
    return "\n".join(lines)[:max_chars]

# --- INTERFACE ---
st.set_page_config(page_title="DELTA AGI", page_icon="üåê", layout="wide")
st.title("üåê DELTA : Syst√®me Jarvis Intelligence Artificielle")

# Sidebar : souvenirs r√©cents
recent_memories = get_recent_memories()
with st.sidebar:
    st.header("üß† M√©moire Vive Jarvis")
    if recent_memories:
        for m in recent_memories:
            st.caption(f"[{m.get('priority','medium')}] {m.get('content')[:50]}...")
    else:
        st.info("Aucun souvenir enregistr√© pour le moment.")
    if st.button("üîÑ Actualiser"):
        recent_memories = get_recent_memories()

# Session state pour le chat
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "√Ä vos ordres, Monsieur Sezer. Jarvis est en ligne. Que souhaitez-vous ?" }]

# Affichage du chat
for m in st.session_state.messages:
    with st.chat_message(m["role"]):
        st.markdown(m["content"])

# --- PROCESSUS PRINCIPAL ---
if prompt := st.chat_input("Parlez √† Jarvis..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # 1Ô∏è‚É£ V√©rification si info utile et cat√©gorisation
    mem_analysis = is_memory_worthy(prompt)
    if mem_analysis.get("is_worthy"):
        branch = mem_analysis.get("branch", "G√©n√©ral")
        priority = mem_analysis.get("priority", "medium")
        m_hash = hash_text(prompt)
        db.collection("users").document(USER_ID).collection("memory").document(m_hash).set({
            "content": prompt,
            "priority": priority,
            "branch": branch,
            "created_at": datetime.utcnow()
        }, merge=True)
        st.toast(f"üß¨ Souvenir m√©moris√© dans {branch} avec priorit√© {priority}")

    # 2Ô∏è‚É£ R√©cup√©ration contexte pour Jarvis
    recent_memories = get_recent_memories()
    context_summary = summarize_context(recent_memories)

    # 3Ô∏è‚É£ R√©ponse Jarvis
    with st.chat_message("assistant"):
        sys_instr = (
            f"Tu es Jarvis, assistant intelligent de Monsieur Sezer. "
            f"Voici les souvenirs r√©cents : {context_summary}. "
            "R√©ponds de fa√ßon concise, intelligente, directe, et toujours pertinente. "
            "Ne mentionne jamais que tu es une IA."
        )
        try:
            response = client.chat.completions.create(
                model="llama-3.3-70b-versatile",
                messages=[{"role": "system", "content": sys_instr}] + st.session_state.messages[-5:]
            ).choices[0].message.content
            st.markdown(response)
            st.session_state.messages.append({"role": "assistant", "content": response})
        except Exception as e:
            st.error(f"Erreur Groq : {e}")
