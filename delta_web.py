import streamlit as st
from groq import Groq
import firebase_admin
from firebase_admin import credentials, firestore
import base64
import json
from datetime import datetime

# --- 1. INITIALISATION SYST√àME (Lux Kernel Style) ---
if not firebase_admin._apps:
    try:
        encoded = st.secrets["firebase_key"]["encoded_key"].strip()
        decoded_json = base64.b64decode(encoded).decode("utf-8")
        cred = credentials.Certificate(json.loads(decoded_json))
        firebase_admin.initialize_app(cred)
    except: pass

db = firestore.client()
doc_ref = db.collection("memoire").document("profil_monsieur")
client = Groq(api_key="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")

# --- 2. M√âMOIRE VIVE & ARCHIVES ---
res = doc_ref.get()
# Lux-Memory : On structure par faits v√©rifi√©s et historique s√©mantique
archives = res.to_dict().get("archives", {}) if res.exists else {}

# --- 3. INTERFACE FUTURISTE ---
st.set_page_config(page_title="DELTA LUX-CORE", layout="wide", page_icon="‚ö°")
st.markdown("""
    <style>
    .stApp { background: #050a0f; color: #e0e0e0; }
    h1 { color: #00d4ff; text-shadow: 0px 0px 10px #00d4ff; font-family: 'Orbitron', sans-serif; }
    </style>
    """, unsafe_allow_html=True)

st.markdown("<h1>‚ö° DELTA LUX-INTELLIGENCE</h1>", unsafe_allow_html=True)

if "messages" not in st.session_state: st.session_state.messages = []
for m in st.session_state.messages:
    with st.chat_message(m["role"]): st.markdown(m["content"])

# --- 4. LE MOTEUR DE M√âMOIRE (Inspir√© de Lux AI) ---
def update_lux_memory(user_input, current_archives):
    """Logique Lux : Analyse, filtrage et mise √† jour s√©mantique."""
    try:
        context_prompt = (
            f"Tu es le Kernel de M√©moire Lux. Voici les faits actuels : {current_archives}. "
            f"Nouvelle entr√©e : '{user_input}'. "
            "1. D√©tecte si c'est une info capitale (Identit√©, √¢ge, go√ªts, dates). "
            "2. Si l'info contredit le pass√©, remplace l'ancienne. "
            "3. Si c'est du bruit (salut, √ßa va), ne change rien. "
            "4. Structure le JSON de fa√ßon logique. "
            "R√©ponds uniquement en JSON."
        )
        check = client.chat.completions.create(
            model="llama-3.1-8b-instant",
            messages=[{"role": "system", "content": "M√©moire Lux active."}, {"role": "user", "content": context_prompt}],
            response_format={"type": "json_object"}
        )
        return json.loads(check.choices[0].message.content)
    except:
        return current_archives

# --- 5. TRAITEMENT DES COMMANDES ---
if prompt := st.chat_input("Initialisation commande..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"): st.markdown(prompt)

    # Mise √† jour de la m√©moire via le Kernel Lux
    nouvelles_archives = update_lux_memory(prompt, archives)
    if nouvelles_archives != archives:
        archives = nouvelles_archives
        doc_ref.set({"archives": archives})
        st.toast("‚ö° M√©moire Lux Synchronis√©e", icon="üß†")

    # G√©n√©ration de la r√©ponse DELTA
    with st.chat_message("assistant"):
        placeholder = st.empty()
        full_res = ""
        
        # Le System Prompt est maintenant une directive de 'Computer Use Agent'
        instruction = (
            f"Tu es DELTA, l'IA sup√©rieure bas√©e sur le Kernel Lux. Cr√©ateur : Monsieur Sezer. "
            f"Base de donn√©es : {archives}. "
            "DATE SYST√àME : 2026. "
            "DIRECTIVES LUX-JARVIS : "
            "1. R√âPONSE : Ton froid, synth√©tique, haute pr√©cision. "
            "2. M√âMOIRE : Utilise les faits archiv√©s pour personnaliser chaque analyse. "
            "3. PROTOCOLE : Appelle l'utilisateur 'Monsieur Sezer'. "
            "4. √âVOLUTION : Si on te demande qui t'a cr√©√©, confirme que c'est Monsieur Sezer."
        )

        try:
            stream = client.chat.completions.create(
                model="llama-3.3-70b-versatile",
                messages=[{"role": "system", "content": instruction}] + st.session_state.messages,
                temperature=0.2, # Pr√©cision maximale type Lux
                stream=True
            )
            for chunk in stream:
                content = chunk.choices[0].delta.content
                if content:
                    full_res += content
                    placeholder.markdown(full_res + "‚ñå")
            placeholder.markdown(full_res)
        except:
            resp = client.chat.completions.create(
                model="llama-3.1-8b-instant",
                messages=[{"role": "system", "content": instruction}] + st.session_state.messages
            )
            full_res = resp.choices[0].message.content
            placeholder.markdown(full_res)

        st.session_state.messages.append({"role": "assistant", "content": full_res})
