import streamlit as st
from groq import Groq
import firebase_admin
from firebase_admin import credentials, firestore
import base64, json, re

# --- CONFIGURATION (Votre Cl√© Groq) ---
GROQ_API_KEY = "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"

# --- CONNEXION FIREBASE (Standard Lux) ---
if not firebase_admin._apps:
    try:
        encoded = st.secrets["firebase_key"]["encoded_key"].strip()
        decoded_json = base64.b64decode(encoded).decode("utf-8")
        cred = credentials.Certificate(json.loads(decoded_json))
        firebase_admin.initialize_app(cred)
    except Exception as e:
        st.error(f"Erreur d'initialisation : {e}")

db = firestore.client()
# Lux s√©pare l'utilisateur par document unique
doc_ref = db.collection("archives").document("monsieur_sezer")
client = Groq(api_key=GROQ_API_KEY)

# --- INITIALISATION DE LA STRUCTURE ---
def get_lux_memory():
    res = doc_ref.get()
    if res.exists:
        return res.to_dict()
    return {
        "profil": {"nom": "Monsieur Sezer", "age": None, "role": "Cr√©ateur"},
        "projets": {},
        "preferences": {},
        "historique_synaptique": []
    }

archives = get_lux_memory()

# --- INTERFACE ---
st.title("DELTA (Engine: LUX-Architecture)")

if "messages" not in st.session_state:
    st.session_state.messages = []

for m in st.session_state.messages:
    with st.chat_message(m["role"]): st.markdown(m["content"])

# --- CORE LOGIC (L'aspiration de Lux) ---
if prompt := st.chat_input("Ordre direct..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"): st.markdown(prompt)

    # 1. LE FILTRE (Extraction par le mod√®le 8B)
    # Lux utilise un 'system prompt' tr√®s strict pour transformer le texte en JSON
    instruction_filtre = (
        "Tu es le processeur de donn√©es de Lux. Ton r√¥le est d'extraire des faits. "
        "Analyse le message de l'utilisateur et renvoie UNIQUEMENT un JSON structur√©. "
        "Si l'utilisateur donne son √¢ge, son nom ou un projet, remplis les cases correspondantes. "
        "Si rien n'est nouveau, r√©ponds '{}'."
    )
    
    analysis = client.chat.completions.create(
        model="llama-3.1-8b-instant",
        messages=[
            {"role": "system", "content": instruction_filtre},
            {"role": "user", "content": f"Message: {prompt} | Archives actuelles: {json.dumps(archives)}"}
        ],
        response_format={"type": "json_object"} # Force le format JSON
    ).choices[0].message.content

    # 2. INJECTION (Sauvegarde Firestore)
    try:
        data_to_save = json.loads(analysis)
        if data_to_save:
            # On fusionne les nouvelles donn√©es avec les anciennes sans rien supprimer
            doc_ref.set(data_to_save, merge=True)
            # Mise √† jour locale pour que l'IA r√©ponde avec les infos fra√Æches
            for key in data_to_save:
                if key in archives: archives[key].update(data_to_save[key])
            st.toast("üß¨ Synapse synchronis√©e")
    except:
        pass

    # 3. R√âPONSE IA (Mod√®le 70B avec la m√©moire de Lux)
    with st.chat_message("assistant"):
        sys_instr = (
            f"Tu es DELTA. Ton cr√©ateur est {archives['profil']['nom']}. "
            f"M√âMOIRE GLOBALE : {json.dumps(archives)}. "
            "TON : Majordome, distingu√©, extr√™mement concis (Style Jarvis). "
            "Anticipe les besoins en fonction des projets et pr√©f√©rences stock√©s."
        )
        
        full_res = client.chat.completions.create(
            model="llama-3.3-70b-versatile",
            messages=[{"role": "system", "content": sys_instr}] + st.session_state.messages[-4:],
        ).choices[0].message.content
        
        st.markdown(full_res)
        st.session_state.messages.append({"role": "assistant", "content": full_res})
