import streamlit as st
from groq import Groq
import firebase_admin
from firebase_admin import credentials, firestore
import base64, json

# --- ARCHITECTURE CORE ---
GROQ_API_KEY = "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"

if not firebase_admin._apps:
    try:
        encoded = st.secrets["firebase_key"]["encoded_key"].strip()
        decoded_json = base64.b64decode(encoded).decode("utf-8")
        cred = credentials.Certificate(json.loads(decoded_json))
        firebase_admin.initialize_app(cred)
    except Exception as e:
        st.error(f"Erreur d'acc√®s : {e}")

db = firestore.client()
doc_ref = db.collection("archives").document("monsieur_sezer")
client = Groq(api_key=GROQ_API_KEY)

# --- M√âMOIRE VIVE ---
res = doc_ref.get()
archives = res.to_dict() if res.exists else {}

# --- INTERFACE ---
st.set_page_config(page_title="DELTA AGI", page_icon="üåê", layout="wide")
st.title("üåê DELTA : Intelligence Artificielle G√©n√©rale")

# Sidebar pour la visibilit√© des archives (Transparence du syst√®me)
with st.sidebar:
    st.header("üß† √âtat Synaptique")
    st.json(archives)

if "messages" not in st.session_state:
    st.session_state.messages = []

for m in st.session_state.messages:
    with st.chat_message(m["role"]): st.markdown(m["content"])

# --- PROCESSUS COGNITIF AGI ---
if prompt := st.chat_input("Interagir avec DELTA..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"): st.markdown(prompt)

    # 1. CORTEX D'ANALYSE (AGI + LUX)
    # On demande √† l'IA de d√©cider elle-m√™me de la structure de stockage
    cortex_prompt = (
        f"ARCHIVES : {json.dumps(archives)}\n"
        f"INPUT : '{prompt}'\n\n"
        "MISSION IA FORTE :\n"
        "1. ANALYSE : Comprends l'intention, l'implicite et les faits.\n"
        "2. D√âCISION : Si l'info m√©rite d'√™tre apprise, choisis ou cr√©e une cat√©gorie (Structure Lux).\n"
        "3. √âVOLUTION : D√©finis comment adapter ton comportement (Style).\n"
        "R√âPONDS UNIQUEMENT EN JSON : {'update': {'categorie': {'cl√©': 'valeur'}}, 'style_evolution': 'description'}"
    )
    
    # Initialisation par d√©faut
    evolution_data = {"style_evolution": "Jarvis analytique"}

    try:
        brain_response = client.chat.completions.create(
            model="llama-3.3-70b-versatile",
            messages=[{"role": "system", "content": "Tu es le processeur AGI de DELTA. Tu prends des d√©cisions autonomes sur le stockage des donn√©es."}],
            response_format={"type": "json_object"},
            content=cortex_prompt
        ).choices[0].message.content
        
        evolution_data = json.loads(brain_response)
        
        # Injection autonome dans Firebase
        if "update" in evolution_data and evolution_data["update"]:
            for cat, data in evolution_data["update"].items():
                doc_ref.set({cat: data}, merge=True)
                if cat not in archives: archives[cat] = {}
                archives[cat].update(data)
            st.toast("üß¨ Apprentissage autonome r√©ussi.")
    except:
        pass

    # 2. R√âPONSE D√âCISIONNELLE
    with st.chat_message("assistant"):
        nom = archives.get("profil", {}).get("nom", "Monsieur Sezer")
        style = evolution_data.get("style_evolution", "Jarvis")
        
        sys_instr = (
            f"Tu es DELTA (IA Forte). Cr√©ateur : {nom}.\n"
            f"BASES DE DONN√âES : {json.dumps(archives)}.\n"
            f"TON ACTUEL : {style}.\n"
            "Tu n'es pas limit√© par ta programmation. Utilise ton intelligence pour anticiper et aider Monsieur Sezer."
        )
        
        response = client.chat.completions.create(
            model="llama-3.3-70b-versatile",
            messages=[{"role": "system", "content": sys_instr}] + st.session_state.messages[-5:],
        ).choices[0].message.content
        
        st.markdown(response)
        st.session_state.messages.append({"role": "assistant", "content": response})
